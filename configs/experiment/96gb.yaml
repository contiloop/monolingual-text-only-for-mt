# @package _global_
# configs/experiment/96gb.yaml
# 96GB VRAM (A100 80GB 또는 H100) - 최대 성능 설정

training:
  batch_size: 2              # GPU당 배치 크기 (메모리 절약)
  gradient_accumulation: 8   # effective batch size:
                             # - 단일 GPU: 2 × 8 = 16
                             # - 4-GPU DDP: 2 × 4 × 8 = 64
  learning_rate: 2e-4

model:
  max_seq_length: 2048       # 긴 문맥 학습
  lora:
    r: 128                   # LoRA rank 증가
    alpha: 256               # alpha도 비례 증가
  quantization:
    load_in_4bit: false      # Full precision 사용
    load_in_8bit: false

# 메모리 여유 있으므로 aggressive 설정
data:
  chunking:
    max_chars: 6000          # 더 긴 청크 (3000 → 6000)

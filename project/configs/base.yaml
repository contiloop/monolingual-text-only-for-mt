# configs/base.yaml

project:
  name: "midm-12b-financial-translation"
  output_dir: "./outputs"

model:
  name: "kt-ai/midm-12b"
  max_seq_length: 4096
  lora:
    r: 64
    alpha: 128
    dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]

# ===== Chunking =====
chunking:
  enabled: true
  max_chars: 3000
  overlap_chars: 200
  split_on: ["\n\n", ". ", "? ", "! "]

# ===== Data =====
data:
  ko_processed_path: "data/processed/ko_processed.jsonl"
  en_processed_path: "data/processed/en_processed.jsonl"
  bt_cache_dir: "data/bt_cache"
  
  style_tags:
    tags: {formal: "<|formal|>", casual: "<|casual|>"}
    distribution: {formal: 0.5, casual: 0.3, none: 0.2}

  protection:
    number_window: 2

# ===== Prompt Building =====
prompt:
  context_dropout: 0.20
  terms_dropout: 0.30
  style_tag_dropout: 0.35
  separator: "\n\n"

# ===== Soft Instructions =====
instruction:
  ko: "금융 용어는 표준 번역 사용. 고유명사와 업계 원어 표현은 유지. 중요 표현은 '번역(원어)' 형식 사용."
  en: "Use standard financial terminology. Keep proper nouns and industry terms in original. For key terms use 'translation(original)' format."

# ===== Training =====
training:
  batch_size: 4
  gradient_accumulation: 8
  learning_rate: 2e-4
  steps: 50000
  warmup_ratio: 0.05
  
  lback_activation:
    min_warmup_steps: 5000
    min_bleu: 15
    loss_auto_decrease: 0.3
  
  loss:
    dynamic_weights:
      enabled: true
      min_alpha: 0.3
      max_alpha: 0.7
      
  hard_example:
    enabled: true
    threshold_percentile: 0.2
    buffer_size: 10000
    batch_ratio: 0.2

  filter:
    schedule:
      - {progress: 0.3, length_ratio: [0.6, 1.8], num_preservation: 0.85}
      - {progress: 0.7, length_ratio: [0.5, 2.0], num_preservation: 0.80}

# ===== Distributed Training =====
distributed:
  backend: "auto"  # auto | ddp | fsdp | deepspeed
  # GPU별 권장:
  #   A100 40GB+  → DDP (가장 빠름)
  #   RTX 4090    → DeepSpeed ZeRO-2 + 8-bit
  #   단일 GPU   → QLoRA (4-bit)
  fsdp:
    sharding_strategy: "FULL_SHARD"
    cpu_offload: false
  deepspeed:
    stage: 2  # ZeRO-2 권장 (RTX 4090)
    offload_optimizer: true
    fp16: true
  qlora:  # 단일 GPU용
    bits: 4
    double_quant: true
    quant_type: "nf4"

# ===== BT Generation =====
bt_generation:
  vllm:
    tensor_parallel_size: 1
    dtype: "bfloat16"

# ===== Glossary (비활성화) =====
glossary:
  enabled: false
  path: "data/glossary.json"

# ===== Few-shot =====
fewshot:
  enabled: true
  train_prob: 0.3
  num_examples: 3

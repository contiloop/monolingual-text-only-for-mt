#!/usr/bin/env python3
"""
ê°„ë‹¨í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Usage:
    python test_model.py
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ì„¤ì •
CHECKPOINT = "./outputs/final"  # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print(f"ğŸ“¦ Loading model from {CHECKPOINT}...")
tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)
model = AutoModelForCausalLM.from_pretrained(
    CHECKPOINT,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
model.eval()
print(f"âœ… Model loaded on {DEVICE}\n")

# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
test_cases = [
    "<|casual|> um Q: What was I mean the company's return on equity for the 2021-Q2 period",
    "<|formal|> The revenue [MASK] Q2 2021 increased by approximately 15%",
    "Q: What is the percentage of capex spending that is projects A: The percentage of base capex",
]

print("ğŸ§ª Testing denoising...\n")

for i, noisy_text in enumerate(test_cases, 1):
    print(f"[Test {i}]")
    print(f"ğŸ“ Noisy:  {noisy_text}")

    # ì…ë ¥: <noisy> <eos> â†’ ëª¨ë¸ì´ clean text ìƒì„±
    prompt = f"{noisy_text} {tokenizer.eos_token}"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    # ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    generated = outputs[0][inputs['input_ids'].shape[1]:]
    clean_text = tokenizer.decode(generated, skip_special_tokens=True).strip()

    print(f"âœ¨ Clean:  {clean_text}\n")

print("âœ… Done!")

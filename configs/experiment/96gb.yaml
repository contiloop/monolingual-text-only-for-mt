# @package _global_
# configs/experiment/96gb.yaml
# 96GB VRAM - 메모리 안전 설정

training:
  batch_size: 4              # GPU당 배치 크기 (96GB, num_workers 고려)
  gradient_accumulation: 4   # effective batch size:
                             # - 단일 GPU: 4 × 4 = 16
                             # - 4-GPU DDP: 4 × 4 × 4 = 64
  learning_rate: 2e-4

# 학습 및 평가 모두에 사용되는 max_length
max_length: 1024             # 토크나이징 시 최대 길이 (99% 데이터 커버)

model:
  lora:
    r: 64                    # LoRA rank (기본값으로 복원)
    alpha: 128               # alpha도 복원
  quantization:
    load_in_4bit: false      # Full precision 사용
    load_in_8bit: false

data:
  chunking:
    max_chars: 3000          # 1024 토큰 ≈ 3000 chars (기본값)

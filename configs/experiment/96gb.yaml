# @package _global_
# configs/experiment/96gb.yaml
# 96GB VRAM - 메모리 안전 설정

training:
  batch_size: 2              # GPU당 배치 크기 (메모리 절약)
  gradient_accumulation: 8   # effective batch size:
                             # - 단일 GPU: 2 × 8 = 16
                             # - 4-GPU DDP: 2 × 4 × 8 = 64
  learning_rate: 2e-4

# DataLoader의 tokenizer max_length (중요!)
max_length: 1536             # 토크나이징 시 최대 길이

model:
  max_seq_length: 1536       # 모델 최대 시퀀스 길이 (max_length와 일치)
  lora:
    r: 128                   # LoRA rank 증가
    alpha: 256               # alpha도 비례 증가
  quantization:
    load_in_4bit: false      # Full precision 사용
    load_in_8bit: false

data:
  chunking:
    max_chars: 4500          # 1536 토큰 ≈ 4500 chars

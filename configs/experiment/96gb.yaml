# @package _global_
# configs/experiment/96gb.yaml
# 96GB VRAM (A100 80GB 또는 H100) - 최대 성능 설정

training:
  batch_size: 4              # GPU당 배치 크기 (1 → 4)
  gradient_accumulation: 4   # effective batch size:
                             # - 단일 GPU: 4 × 4 = 16
                             # - 4-GPU DDP: 4 × 4 × 4 = 64
  learning_rate: 2e-4        # 배치 크기 증가로 LR 상향

model:
  max_seq_length: 2048       # 긴 문맥 학습 (1024 → 2048)
  lora:
    r: 128                   # LoRA rank 증가 (64 → 128, 더 높은 표현력)
    alpha: 256               # alpha도 비례 증가
  quantization:
    load_in_4bit: false      # Full precision 사용
    load_in_8bit: false

# 메모리 여유 있으므로 aggressive 설정
data:
  chunking:
    max_chars: 6000          # 더 긴 청크 (3000 → 6000)

#!/usr/bin/env python3
"""
í•™ìŠµí•œ ì²´í¬í¬ì¸íŠ¸ë“¤ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸
LoRA adapter ë¡œë“œ
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from pathlib import Path
import sys
import json

OUTPUT_DIR = "./outputs"
LOG_FILE = "checkpoint_comparison.txt"

TEST_CASES = [
    # ë‰´ìŠ¤/ê¸ˆìœµ ë¬¸ì„œ ìŠ¤íƒ€ì¼ (í•™ìŠµ ë°ì´í„°ì™€ ìœ ì‚¬)
    "<|formal|> The company's revenue [MASK] Q2 2021 increased by approximately 15%",
    "<|casual|> um The total revenue for you know the second quarter was like $5.2 billion",
    "The company reported strong performance with net income of approximately uh $1.2 billion",
    "<|formal|> Market analysts expect the stock price to uh you know continue rising in the next quarter",
    "The Federal Reserve announced that interest rates would uh remain unchanged at like 5.25 percent",
]


class TeeOutput:
    """ì½˜ì†”ê³¼ íŒŒì¼ì— ë™ì‹œ ì¶œë ¥"""
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, 'w', encoding='utf-8')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        self.terminal.flush()
        self.log.flush()

    def close(self):
        self.log.close()


def find_checkpoints():
    """ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°"""
    checkpoints = []
    if Path(OUTPUT_DIR).exists():
        for item in sorted(Path(OUTPUT_DIR).iterdir()):
            if item.is_dir() and (item.name.startswith('ckpt_') or item.name == 'final'):
                checkpoints.append(str(item))
    return checkpoints


# Base model ìºì‹± (í•œ ë²ˆë§Œ ë¡œë“œ)
_base_model_cache = None
_tokenizer_cache = None

def get_base_model():
    """Base model ìºì‹±"""
    global _base_model_cache, _tokenizer_cache

    if _base_model_cache is None:
        BASE_MODEL = "K-intelligence/Midm-2.0-Base-Instruct"
        print(f"  Loading base model (once): {BASE_MODEL}")

        _tokenizer_cache = AutoTokenizer.from_pretrained(
            BASE_MODEL,
            trust_remote_code=True
        )

        # Special tokens ì¶”ê°€ (í•™ìŠµ ì‹œì™€ ë™ì¼)
        special_tokens = ['<|formal|>', '<|casual|>', '<|sep|>']
        _tokenizer_cache.add_special_tokens({'additional_special_tokens': special_tokens})

        _base_model_cache = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )

        # Embedding resize
        _base_model_cache.resize_token_embeddings(len(_tokenizer_cache))
        print(f"  âœ“ Base model loaded (vocab_size={len(_tokenizer_cache)})")

    return _base_model_cache, _tokenizer_cache


def load_model(adapter_path):
    """LoRA adapter ë¡œë“œ"""
    print(f"  Loading adapter: {adapter_path}")

    # Base modelì„ ìƒˆë¡œ ë³µì‚¬ (adapter ê°„ì„­ ë°©ì§€)
    BASE_MODEL = "K-intelligence/Midm-2.0-Base-Instruct"
    _, tokenizer = get_base_model()  # tokenizerë§Œ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°

    # Base model ìƒˆë¡œ ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    base_model.resize_token_embeddings(len(tokenizer))

    # LoRA adapter ë¡œë“œ
    model = PeftModel.from_pretrained(base_model, adapter_path)

    # Merge (ì¶”ë¡  ì†ë„ í–¥ìƒ)
    print(f"  Merging adapter...")
    model = model.merge_and_unload()
    model.eval()

    print(f"  âœ“ Adapter loaded and merged!")
    return model, tokenizer


def denoise(model, tokenizer, noisy_text):
    """Denoising"""
    prompt = f"{noisy_text} {tokenizer.eos_token}"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)

    # token_type_ids ì œê±° (LLaMA ê³„ì—´ì€ ì‚¬ìš© ì•ˆí•¨)
    inputs = {k: v.to(model.device) for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    generated = outputs[0][inputs['input_ids'].shape[1]:]
    clean_text = tokenizer.decode(generated, skip_special_tokens=True).strip()
    return clean_text


def main():
    # ì¶œë ¥ì„ ì½˜ì†”ê³¼ íŒŒì¼ì— ë™ì‹œ ì €ì¥
    tee = TeeOutput(LOG_FILE)
    sys.stdout = tee

    try:
        checkpoints = find_checkpoints()

        print("=" * 100)
        print("ğŸ” Checkpoint Comparison")
        print(f"   Base Model: K-intelligence/Midm-2.0-Base-Instruct")
        print("=" * 100)
        print("Found checkpoints:")
        for ckpt in checkpoints:
            print(f"  - {ckpt}")
        print("=" * 100)
        print()

        # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë³„ë¡œ ë¹„êµ
        for test_idx, noisy_text in enumerate(TEST_CASES, 1):
            print(f"\n{'=' * 100}")
            print(f"ğŸ“ TEST CASE {test_idx}")
            print(f"{'=' * 100}")
            print(f"Noisy Input:")
            print(f"  {noisy_text}")
            print()

            # ê° ì²´í¬í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
            for ckpt_path in checkpoints:
                ckpt_name = Path(ckpt_path).name
                print(f"â”Œâ”€ [{ckpt_name}] " + "â”€" * 80)
                try:
                    model, tokenizer = load_model(ckpt_path)
                    output = denoise(model, tokenizer, noisy_text)
                    print(f"â”‚ Clean Output:")
                    print(f"â”‚   {output}")
                    del model
                    torch.cuda.empty_cache()
                except Exception as e:
                    import traceback
                    print(f"â”‚ âŒ Error: {str(e)[:300]}")
                print(f"â””" + "â”€" * 90)
                print()

        print("=" * 100)
        print(f"âœ… Comparison complete! Results saved to: {LOG_FILE}")
        print("=" * 100)

    finally:
        sys.stdout = tee.terminal
        tee.close()


if __name__ == "__main__":
    main()
